lhood.fn <- doc.lhood.fn(lambda)
cl
install.packages("iFad")
system("defaults write org.R-project.R force.LANG en_US.UTF-8")
install.packages("ggplot2")
install.packages('ggplot2', dependencies=TRUE, repos='http://cran.rstudio.com/')
install.packages("ggplot2")
install.packages("ggplot2",lib="/data/Rpackages/")
options(repos='http://cran.rstudio.com/')
install.packages("ggplot2")
install.packages("randomForest", repos="http://cran.cnr.berkeley.edu")
install.packages()
ls
isfar<-load("/Users/sikun/Documents/MATLAB/code_FA/dataset/RE.RData")
isfar<-load("/Users/sikun/Documents/MATLAB/code_FA/dataset/NCI-60 data/RE.RData")
isfar
isfar<-load("/Users/sikun/Documents/MATLAB/code_FA/dataset/NCI-60 data/OV.RData")
isfar
head(isfar)
ls()
isfar<-get(load("/Users/sikun/Documents/MATLAB/code_FA/dataset/NCI-60 data/OV.RData"))
isfar
isfar<-get(load("/Users/sikun/Documents/MATLAB/code_FA/dataset/NCI-60 data/prior_L1_L2.RData"))
isfar
var<-get(load("/Users/sikun/Documents/MATLAB/code_FA/dataset/NCI-60 data/prior_L1_L2.RData"))
var
write.csv(var, file = "/Users/sikun/Documents/MATLAB/code_FA/dataset/NCI-60 data/prior_L1_L2_01", append = FALSE, quote = TRUE, sep = " ",#
            eol = "\n", na = "NA", dec = ".", row.names = TRUE,#
            col.names = TRUE, qmethod = c("escape", "double"),#
            fileEncoding = "")
write.csv(var, file = "", append = FALSE, quote = TRUE, sep = " ",#
            eol = "\n", na = "NA", dec = ".", row.names = TRUE,#
            col.names = TRUE, qmethod = c("escape", "double"),#
            fileEncoding = "")
write.csv(var, file = "/Users/sikun/Documents/MATLAB/code_FA/dataset/NCI-60 data/prior_L1_L2_01.csv", append = FALSE, quote = TRUE, sep = " ",eol = "\n", na = "NA", dec = ".", row.names = TRUE,col.names = TRUE, qmethod = c("escape", "double"),fileEncoding = "")
write.csv(var, file = "/Users/sikun/Documents/MATLAB/code_FA/dataset/NCI-60 data/prior_L1_L2_01.csv")
write.csv(var, file = "/Users/sikun/Documents/MATLAB/code_FA/dataset/NCI-60 data/prior_L1_L2_01.csv", append = FALSE, quote = TRUE, sep = " ",eol = "\n", na = "NA", dec = ".", row.names = TRUE,col.names = TRUE, qmethod = c("escape", "double"))
ls
ls()
class(var)
length(var)
write.table(var, file = "/Users/sikun/Documents/MATLAB/code_FA/dataset/NCI-60 data/prior_L1_L2_01.txt", sep = "\t ")
var<-get(load("/Users/sikun/Documents/MATLAB/code_FA/dataset/NCI-60 data/RE.RData"))
var
class(var)
write.table(var, file = "/Users/sikun/Documents/MATLAB/code_FA/dataset/NCI-60 data/RE.txt", sep = "\t ")
var<-get(load("/Users/sikun/Documents/MATLAB/code_FA/dataset/iFad/data/matrixL1.rda"))
var
var<-get(load("/Users/sikun/Documents/MATLAB/download/duke_BCC/BRCAData.rda"))
var<-get(load("/Users/sikun/Documents/MATLAB/code_FA/dataset/iFad/data/matrixL1.rda"))
var
var<-get(load("/Users/sikun/Documents/MATLAB/download/duke_BCC/Results.rda"))
var<-(load("/Users/sikun/Documents/MATLAB/download/duke_BCC/Results.rda"))
GE <- read.csv("/Users/sikun/Documents/MATLAB/download/duke_BCC/BRCA.exp.348.med.csv.gz", row.names = 1)
miRNA <- read.csv("/Users/sikun/Documents/MATLAB/download/duke_BCC/BRCA.348.precursor.txt.gz", row.names = 1)
require(impute)
require(gtools)
install.packages("master.tar.gz", type = "source", repos = NULL)
var<-get(load("/Users/sikun/Documents/MATLAB/download/duke_BCC/BRCAData.rda"))
var<-load("/Users/sikun/Documents/MATLAB/download/duke_BCC/BRCAData.rda")
plotModel <- function(toy,res,predY) {
genData <- function(V=1) {
source("/Users/sikun/Documents/MATLAB/download/Aalto_PML/GFA/GFA/demo/GFAexample.R")
demo()
require('CCAGFA')
Ntrain <- Ntest <- 100
N <- Ntrain + Ntest
D <- c(15,7)
D
N
K <- 4
K
Z <- matrix(0,N,K)
Z
cl
K
Z[,1] <- sin((1:N)/(N/20))#
Z[,2] <- cos((1:N)/(N/20))#
Z[,3] <- rnorm(N,0,1)#
Z[,4] <- as.matrix(c(2*((1:Ntrain)/Ntrain-0.5),2*((1:Ntest)/Ntest-0.5)),N,1)
Z
tau <- c(3,6)
alpha <- matrix(0,2,4)    # Component precisions for the two data sets#
alpha[1,] <- c(1,1,1e6,1) # 1   = active (the value determines the data scale)#
alpha[2,] <- c(1,1,1,1e6)
Y <- vector("list",length=2)#
Ytest <- vector("list",length=2)
Y
Ytest
W <- vector("list",length=2)#
for(view in 1:2) {#
  W[[view]] <- matrix(0,D[view],K)#
  for(k in 1:K) {#
    W[[view]][,k] <- rnorm(D[view],0,1/sqrt(alpha[view,k]))#
  }#
  Y[[view]] <- Z %*% t(W[[view]]) +#
    matrix(rnorm(N*D[view],0,1/sqrt(tau[view])),N,D[view])#
#
  Ytest[[view]] <- Y[[view]][(Ntrain+1):N,]#
  Y[[view]] <- Y[[view]][1:Ntrain,]#
}#
Ztest <- Z[(Ntrain+1):N,]#
Z <- Z[1:Ntrain,]
W
Y
W
Y
Z
W
Y <- vector("list",length=2)#
Ytest <- vector("list",length=2)#
W <- vector("list",length=2)#
for(view in 1:2) {#
  W[[view]] <- matrix(0,D[view],K)#
  for(k in 1:K) {#
    W[[view]][,k] <- rnorm(D[view],0,1/sqrt(alpha[view,k]))#
  }#
  Y[[view]] <- Z %*% t(W[[view]]) +#
    matrix(rnorm(N*D[view],0,1/sqrt(tau[view])),N,D[view])#
#
  Ytest[[view]] <- Y[[view]][(Ntrain+1):N,]#
  Y[[view]] <- Y[[view]][1:Ntrain,]#
}
Y <- vector("list",length=2)#
Ytest <- vector("list",length=2)#
W <- vector("list",length=2)#
for(view in 1:2) {#
  W[[view]] <- matrix(0,D[view],K)#
  for(k in 1:K) {#
    W[[view]][,k] <- rnorm(D[view],0,1/sqrt(alpha[view,k]))#
  }#
  Y[[view]] <- Z %*% t(W[[view]]) +#
    matrix(rnorm(N*D[view],0,1/sqrt(tau[view])),N,D[view])#
#
  Ytest[[view]] <- Y[[view]][(Ntrain+1):N,]#
  Y[[view]] <- Y[[view]][1:Ntrain,]#
}#
Ztest <- Z[(Ntrain+1):N,]#
Z <- Z[1:Ntrain,]
require('CCAGFA')
Ntrain <- Ntest <- 100   # Number of samples (you can try also#
                         #   smaller and larger values)#
N <- Ntrain + Ntest#
D <- c(15,7)             # Data dimensions#
K <- 4#
Z <- matrix(0,N,K)       # Latent components#
Z[,1] <- sin((1:N)/(N/20))#
Z[,2] <- cos((1:N)/(N/20))#
Z[,3] <- rnorm(N,0,1)#
Z[,4] <- as.matrix(c(2*((1:Ntrain)/Ntrain-0.5),2*((1:Ntest)/Ntest-0.5)),N,1)#
tau <- c(3,6)             # Noise precisions#
alpha <- matrix(0,2,4)    # Component precisions for the two data sets#
alpha[1,] <- c(1,1,1e6,1) # 1   = active (the value determines the data scale)#
alpha[2,] <- c(1,1,1,1e6)
Y <- vector("list",length=2)#
Ytest <- vector("list",length=2)#
W <- vector("list",length=2)#
for(view in 1:2) {#
  W[[view]] <- matrix(0,D[view],K)#
  for(k in 1:K) {#
    W[[view]][,k] <- rnorm(D[view],0,1/sqrt(alpha[view,k]))#
  }#
  Y[[view]] <- Z %*% t(W[[view]]) +#
    matrix(rnorm(N*D[view],0,1/sqrt(tau[view])),N,D[view])#
#
  Ytest[[view]] <- Y[[view]][(Ntrain+1):N,]#
  Y[[view]] <- Y[[view]][1:Ntrain,]#
}
Z
Ztest <- Z[(Ntrain+1):N,]#
Z <- Z[1:Ntrain,]
objects()
K <- 8
opts <- getDefaultOpts()#
opts$iter.crit <- 1e-6     # Need to make this more strict if#
                           #   having a large sample size#
opts$lbfgs.factr <- 1e5    # A bit less strict convergence criterion,#
                           #   should be enough for our simple dataset#
opts$verbose <- 1          # Looks a bit nicer#
print("Training the model")#
print("==================")#
model <- CCAexperiment(Y,K,opts)
print("")#
print("Noise variance check")#
print("====================")#
print(paste("True      :",paste(format(1/tau,digits=2),collapse=",")))#
print(paste("Estimated :",paste(format(1/model$tau,digits=2),collapse=",")))#
print("")
model
print("Prediction check (relative MSE for each output dimension)")#
print("=========================================================")#
for(m in 1:2) {#
  if(m==1) {#
    observed <- c(1,0)#
    mpred <- 2#
    print("Predicting from view 1 to view 2.")#
  } else {#
    observed <- c(0,1)#
    mpred <- 1#
    print("Predicting from view 2 to view 1.")#
  }#
  #
  pred <- CCApred(observed,Ytest,model)#
  error <- vector()#
  for(d in 1:D[mpred]) {#
    error[d] <- mean((Ytest[[mpred]][,d]-pred$Y[[mpred]][,d])^2) / mean(Ytest[[mpred]][,d]^2)#
  }#
  print(paste("Full model    :",paste(format(error,digits=2),collapse=",")))#
  #
  # Show that the predictions are the same when inactive components#
  # are removed, so they really were inactive#
  trimmed <- CCAtrim(model)#
  pred2 <- CCApred(observed,Ytest,trimmed)#
  error2 <- vector()#
  for(d in 1:D[mpred]) {#
    error2[d] <- mean((Ytest[[mpred]][,d]-pred2$Y[[mpred]][,d])^2) / mean(Ytest[[mpred]][,d]^2)#
  }#
  print(paste("Trimmed model :",paste(format(error2,digits=2),collapse=",")))#
#
  # Feature-wise linear regression for comparison#
  error3 <- vector()#
  for(d in 1:D[mpred]) {#
    xnam <- paste(paste("X",1:D[m],sep=""),collapse="+")#
    fit <- lm(paste("Y[[mpred]][,d] ~ ",xnam),data=data.frame(Y[[m]]))#
    out <- predict.lm(fit,newdata=data.frame(Ytest[[m]]))#
    error3[d] <- mean((Ytest[[mpred]][,d]-out)^2) / mean(Ytest[[mpred]][,d]^2)#
  }#
  print(paste("Least squares :",paste(format(error3,digits=2),collapse=",")))#
  print("")#
#
#X11();#
#  barplot(rbind(error,error2,error3),beside=TRUE,legend.text=c("BCCA/BIBFA",#
                                                   "trimmed BCCA/BIBFA",#
                                                   "Regression"))#
  title(paste("Prediction errors for the features of data set",mpred))#
}
print("Prediction check (relative MSE for each output dimension)")#
print("=========================================================")#
for(m in 1:2) {#
  if(m==1) {#
    observed <- c(1,0)#
    mpred <- 2#
    print("Predicting from view 1 to view 2.")#
  } else {#
    observed <- c(0,1)#
    mpred <- 1#
    print("Predicting from view 2 to view 1.")#
  }#
  #
  pred <- CCApred(observed,Ytest,model)#
  error <- vector()#
  for(d in 1:D[mpred]) {#
    error[d] <- mean((Ytest[[mpred]][,d]-pred$Y[[mpred]][,d])^2) / mean(Ytest[[mpred]][,d]^2)#
  }#
  print(paste("Full model    :",paste(format(error,digits=2),collapse=",")))#
  #
  # Show that the predictions are the same when inactive components#
  # are removed, so they really were inactive#
  trimmed <- CCAtrim(model)#
  pred2 <- CCApred(observed,Ytest,trimmed)#
  error2 <- vector()#
  for(d in 1:D[mpred]) {#
    error2[d] <- mean((Ytest[[mpred]][,d]-pred2$Y[[mpred]][,d])^2) / mean(Ytest[[mpred]][,d]^2)#
  }#
  print(paste("Trimmed model :",paste(format(error2,digits=2),collapse=",")))#
#
  # Feature-wise linear regression for comparison#
  error3 <- vector()#
  for(d in 1:D[mpred]) {#
    xnam <- paste(paste("X",1:D[m],sep=""),collapse="+")#
    fit <- lm(paste("Y[[mpred]][,d] ~ ",xnam),data=data.frame(Y[[m]]))#
    out <- predict.lm(fit,newdata=data.frame(Ytest[[m]]))#
    error3[d] <- mean((Ytest[[mpred]][,d]-out)^2) / mean(Ytest[[mpred]][,d]^2)#
  }#
  print(paste("Least squares :",paste(format(error3,digits=2),collapse=",")))#
  print("")}
model
N
nrow(Y[[tr[1]]])
nrow(Y[[1]])
pred
require('CCAGFA')#
#
##
# Generate some data from the model, with pre-specified#
# latent components#
##
Ntrain <- Ntest <- 100   # Number of samples (you can try also#
                         #   smaller and larger values)#
N <- Ntrain + Ntest#
D <- c(15,7)             # Data dimensions#
K <- 4#
Z <- matrix(0,N,K)       # Latent components#
Z[,1] <- sin((1:N)/(N/20))#
Z[,2] <- cos((1:N)/(N/20))#
Z[,3] <- rnorm(N,0,1)#
Z[,4] <- as.matrix(c(2*((1:Ntrain)/Ntrain-0.5),2*((1:Ntest)/Ntest-0.5)),N,1)#
tau <- c(3,6)             # Noise precisions#
alpha <- matrix(0,2,4)    # Component precisions for the two data sets#
alpha[1,] <- c(1,1,1e6,1) # 1   = active (the value determines the data scale)#
alpha[2,] <- c(1,1,1,1e6) # 1e6 = inactive#
#
# Create some random projection vectors and sample data from the#
# model. Note that CCAGFAtools.R has a function for sampling data#
# from a full model, but as we do not yet have a model we create#
# the toy data here.#
Y <- vector("list",length=2)#
Ytest <- vector("list",length=2)#
W <- vector("list",length=2)#
for(view in 1:2) {#
  W[[view]] <- matrix(0,D[view],K)#
  for(k in 1:K) {#
    W[[view]][,k] <- rnorm(D[view],0,1/sqrt(alpha[view,k]))#
  }#
  Y[[view]] <- Z %*% t(W[[view]]) +#
    matrix(rnorm(N*D[view],0,1/sqrt(tau[view])),N,D[view])#
#
  Ytest[[view]] <- Y[[view]][(Ntrain+1):N,]#
  Y[[view]] <- Y[[view]][1:Ntrain,]#
}#
Ztest <- Z[(Ntrain+1):N,]#
Z <- Z[1:Ntrain,]#
#
##
# Run CCA/BIBFA#
##
K <- 8  # The number of components; should be high enough to capture#
        # all of the components. This can be recognized by at least a few#
        # of the components being shut down#
opts <- getDefaultOpts()#
opts$iter.crit <- 1e-6     # Need to make this more strict if#
                           #   having a large sample size#
opts$lbfgs.factr <- 1e5    # A bit less strict convergence criterion,#
                           #   should be enough for our simple dataset#
opts$verbose <- 1          # Looks a bit nicer#
print("Training the model")#
print("==================")#
model <- CCAexperiment(Y,K,opts)
model
D
model.covW
covW
model$covW
model$covW[1]
model$covW[1]*D[1]
D[1]
model$covW[1]*D [1]
model$covW[1]*D
model
model$tau
print("Prediction check (relative MSE for each output dimension)")#
print("=========================================================")#
for(m in 1:2) {#
  if(m==1) {#
    observed <- c(1,0)#
    mpred <- 2#
    print("Predicting from view 1 to view 2.")#
  } else {#
    observed <- c(0,1)#
    mpred <- 1#
    print("Predicting from view 2 to view 1.")#
  }#
  #
  pred <- CCApred(observed,Ytest,model)#
  error <- vector()#
  for(d in 1:D[mpred]) {#
    error[d] <- mean((Ytest[[mpred]][,d]-pred$Y[[mpred]][,d])^2) / mean(Ytest[[mpred]][,d]^2)#
  }#
  print(paste("Full model    :",paste(format(error,digits=2),collapse=",")))#
  #
  # Show that the predictions are the same when inactive components#
  # are removed, so they really were inactive#
  trimmed <- CCAtrim(model)#
  pred2 <- CCApred(observed,Ytest,trimmed)#
  error2 <- vector()#
  for(d in 1:D[mpred]) {#
    error2[d] <- mean((Ytest[[mpred]][,d]-pred2$Y[[mpred]][,d])^2) / mean(Ytest[[mpred]][,d]^2)#
  }#
  print(paste("Trimmed model :",paste(format(error2,digits=2),collapse=",")))#
#
  # Feature-wise linear regression for comparison#
  error3 <- vector()#
  for(d in 1:D[mpred]) {#
    xnam <- paste(paste("X",1:D[m],sep=""),collapse="+")#
    fit <- lm(paste("Y[[mpred]][,d] ~ ",xnam),data=data.frame(Y[[m]]))#
    out <- predict.lm(fit,newdata=data.frame(Ytest[[m]]))#
    error3[d] <- mean((Ytest[[mpred]][,d]-out)^2) / mean(Ytest[[mpred]][,d]^2)#
  }#
  print(paste("Least squares :",paste(format(error3,digits=2),collapse=",")))#
  print("")}
Ytest[mpred]
Ytest[1][,1]
Ytest[1][1]
Ytest[[1]][1]
Ytest[[1]]
Ytest[[1]][,1]
pred2$Y[[1]][,1]
require('CCAGFA')
Ntrain <- Ntest <- 100   # Number of samples (you can try also#
                         #   smaller and larger values)#
N <- Ntrain + Ntest#
D <- c(15,7)             # Data dimensions#
K <- 4#
Z <- matrix(0,N,K)       # Latent components#
Z[,1] <- sin((1:N)/(N/20))#
Z[,2] <- cos((1:N)/(N/20))#
Z[,3] <- rnorm(N,0,1)#
Z[,4] <- as.matrix(c(2*((1:Ntrain)/Ntrain-0.5),2*((1:Ntest)/Ntest-0.5)),N,1)#
tau <- c(3,6)             # Noise precisions#
alpha <- matrix(0,2,4)    # Component precisions for the two data sets#
alpha[1,] <- c(1,1,1e6,1) # 1   = active (the value determines the data scale)#
alpha[2,] <- c(1,1,1,1e6) # 1e6 = inactive#
#
# Create some random projection vectors and sample data from the#
# model. Note that CCAGFAtools.R has a function for sampling data#
# from a full model, but as we do not yet have a model we create#
# the toy data here.#
Y <- vector("list",length=2)#
Ytest <- vector("list",length=2)#
W <- vector("list",length=2)#
for(view in 1:2) {#
  W[[view]] <- matrix(0,D[view],K)#
  for(k in 1:K) {#
    W[[view]][,k] <- rnorm(D[view],0,1/sqrt(alpha[view,k]))#
  }#
  Y[[view]] <- Z %*% t(W[[view]]) +#
    matrix(rnorm(N*D[view],0,1/sqrt(tau[view])),N,D[view])#
#
  Ytest[[view]] <- Y[[view]][(Ntrain+1):N,]#
  Y[[view]] <- Y[[view]][1:Ntrain,]#
}#
Ztest <- Z[(Ntrain+1):N,]#
Z <- Z[1:Ntrain,]
Y
K <- 8  # The number of components; should be high enough to capture#
        # all of the components. This can be recognized by at least a few#
        # of the components being shut down#
opts <- getDefaultOpts()#
opts$iter.crit <- 1e-6     # Need to make this more strict if#
                           #   having a large sample size#
opts$lbfgs.factr <- 1e5    # A bit less strict convergence criterion,#
                           #   should be enough for our simple dataset#
opts$verbose <- 1          # Looks a bit nicer#
print("Training the model")#
print("==================")#
model <- CCAexperiment(Y,K,opts)
require("GFA")
genData <- function(V=1) {#
  if(V==1) print("Generating three data views paired in one mode with 6 group sparse components.")#
  if(V==2) {#
    print(paste("Generating data with pairing in two modes, with a total of 4",#
                "data views. 4 bicluster components."))#
  }#
  N <- c(200,100) #Number of samples#
  D <- list(c(100,50,60),c(200,70)) #Dimensions#
  K <- c(2,2); if(V==1) K <- 6 #Component numbers#
  Nzero <- list(list(1:100,101:200),list(1:50,51:100))#
  if(V==1) Dzero <- list(list(c(),1:100,101:150,151:210,1:150,101:210))#
  else Dzero <- list(list(1:100,c(1:30,151:210)),list(1:200,c(1:140,250:270)))#
  #
  Y <- y <- X <- W <- list()#
  for(v in 1:V) {#
    X[[v]] <- matrix(rnorm(N[v]*K[v]),N[v],K[v]) #Generate the latents#
    W[[v]] <- matrix(rnorm(sum(D[[v]]*K[v])),sum(D[[v]]),K[v]) #Generate the projections#
    if(V==1) { #Varying scale to components#
      W[[v]] <- scale(W[[v]]); W[[v]] <- sweep(W[[v]],MARGIN=2,c(.8, 2.2, 1.4, 0.7, 1.7, 0.5),"*") #
    }#
#
    for(k in 1:K[v]) { #Sparsity#
      if(V==2) X[[v]][Nzero[[v]][[k]],k] <- 0 #Sample sparsity only with biclustering#
      if(length(Dzero[[v]][[k]])>0) W[[v]][Dzero[[v]][[k]],k] <- 0#
    }#
    #
    tau <- 1 #tau <- 0.2; if(V==1) tau <- 2#
    Y[[v]] <- tcrossprod(X[[v]],W[[v]]) + matrix(rnorm(N[v]*sum(D[[v]]),0,tau),N[v],sum(D[[v]]))#
    y[[v]] <- list()#
    d <- 0#
    for(m in 1:length(D[[v]])) {#
      y[[v]][[m]] <- Y[[v]][,1:D[[v]][m]+d]#
      if(v==2) { #Effect of both component sets to the shared view#
        kk <- 1:K[v]#
        y[[2]][[1]] <- t(y[[1]][[1]]) + tcrossprod(X[[v]][,kk],W[[v]][1:D[[2]][1],kk])#
        y[[1]][[1]] <- t(y[[2]][[1]])#
      }#
      d <- d + D[[v]][m]#
    }#
  }#
  groups <- list()#
  for(v in 1:V) {#
    groups[[v]] <- list()#
    d <- c(0, cumsum(D[[v]]))#
    for(m in 1:(length(d)-1)) groups[[v]][[m]] <- (d[m]+1):d[m+1]#
  }#
#
  yNew <- YNew <- NA#
  ntest <- 1:20 # First 20 samples for test use#
  if(V==1) {#
    YNew <- Y[[1]][ntest,]; Y[[1]] <- Y[[1]][-ntest,]#
    yNew <- list(list())#
    for(m in 1:length(y[[1]])) {#
      yNew[[1]][[m]] <- y[[1]][[m]][ntest,]; y[[1]][[m]] <- y[[1]][[m]][-ntest,]#
    }#
  }#
  #
  return(list(y=y,Y=Y,X=X,W=W,N=N,D=D,K=K,groups=groups,yNew=yNew,YNew=YNew))#
}
Y
#Function for plotting a data collection paired in two modes#
showComp <- function(toy,res=NULL) { #Y,main,D1,D2) {#
  D1 <- cumsum(toy$D[[1]])#
  D2 <- cumsum(toy$D[[2]])#
  Y <- list()#
  #
  if(is.null(res)) {#
    for(v in 1:2) {#
      for(k in 1:toy$K[v]) {#
        Y[[length(Y)+1]] <- tcrossprod(toy$X[[v]][,k],toy$W[[v]][,k])#
      }#
    }#
    main <- "True bicluster components"#
    cat("True bicluster components:#
  similar to a toy data experiment in Bunte et al., gray rectangle denote data#
  sources. Various colors denote different components, each active in a subset#
  of the samples and the features (bicluster).")#
    #
  } else {#
    for(v in 1:2) {#
      for(k in 1:res$K[v]) {#
        X <- matrix(0,nrow(res$X[[v]]),nrow(res$W[[v]]))#
        for(i in 1:opts$iter.saved)#
          X <- X + tcrossprod(res$posterior$X[[v]][i,,k], res$posterior$W[[v]][i,,k])#
        Y[[length(Y)+1]] <- X/opts$iter.saved#
      }#
    }#
    main <- "Inferred components"#
    cat("Inferred components:#
  the decomposition GFA has inferred, matching closely to the true structure.#
  The components have not been ordered, so the colors may vary.")#
  }#
  #
  K <- length(Y)#
  X <- matrix(0,max(D2),max(D1))#
  for(k in 1:K) {#
    N <- nrow(Y[[k]])#
    D <- ncol(Y[[k]])#
    x <- abs(Y[[k]])+k*10#
    if(N==D2[1]) {#
      id <- which(x%%10 >= X[1:N,1:D]%%10,arr.ind=T)#
      X[id] <- x[id] #Showing only the strongest component#
    } else {#
      id <- which(t(x)%%10 >= X[1:D,1:N]%%10)#
      X[id] <- t(x)[id]#
    }#
  }#
  X[is.na(X)] <- 10 #Base level (gray)#
  X[-1:-(D2[1]),-1:-(D1[1])] <- NA #Top right white (no data)#
  #
  #Separate the data sets#
  s <- 20 #Whitespace#
  Y <- X[1:(D2[1]*1),1:(D1[1]*1)]#
  for(i in 2:length(D1))#
    Y <- cbind(Y,Y <- matrix(NA,D2[1]*1,s),X[1:(D2[1]*1),(D1[i-1]*1+1):(D1[i]*1)])#
  Y <- rbind(Y,matrix(NA,(D2[2]-D2[1])*1+s,ncol(Y)))#
  Y[(D2[1]*1+1):(D2[2]*1)+s,1:(D1[1]*1)] <- X[(D2[1]*1+1):(D2[2]*1),1:(D1[1]*1)]#
  #
  cc <- c("red","blue","green","orange","cyan","brown","yellow","black")#
  cols <- "grey"#
  for(k in 1:K) cols <- c(cols,rep(cc[k],2),"grey")#
  cols <- colorRampPalette(cols)(50*K)#
  at <- c()#
  for(k in 1:K)#
    at <- c(at,seq(0,6,length=50)+k*10)#
  at <- c(at,(K+1)*10)#
  #
  image(t(Y),col=cols,breaks=at,axes=F,main=main)#
  return(X)#
}
set.seed(12345)#
toy <- genData(V=1) #Data generation#
opts <- getDefaultOpts(bicluster=FALSE) #Model options#
res <- gfa(toy$y[[1]],opts=opts,K=10) #Model inferece
toy <- genData(V=1)
opts <- getDefaultOpts(bicluster=FALSE)
res <- gfa(toy$y[[1]],opts=opts,K=10)
?write
?paste0
res <- gfa(toy$y[[1]],opts=opts,K=10)
dim(toy$y[[1]])
length(toy$y[[1]])
 X <- matrix(rnorm(20*3),20,3)                     #Latent variables#
W <- matrix(rnorm(30*3),30,3)                     #Projection matrix#
Y <- tcrossprod(X,W) + matrix(rnorm(20*30),20,30) #Observations#
Y <- sweep(Y, MARGIN=2, runif(30), "+")           #Feature means#
Y <- list(Y[,1:10], Y[,11:30])                    #Data grouping#
#Model inference and visualization#
norm <- normalizeData(Y, type="center")           #Centering#
opts <- getDefaultOpts()                          #Model options#
#Fast runs for the demo, default options recommended in general#
opts[c("iter.burnin", "iter.max")] <- c(500, 1000)#
res <- gfa(norm$train, K=5, opts=opts)            #Model inference
?gfa
dim(X)
dim(W)
dim(Y)
Y <- sweep(Y, MARGIN=2, runif(30), "+")
Y <- sweep(Y,MARGIN=2,runif(30),"+")
Y <- tcrossprod(X,W) + matrix(rnorm(20*30),20,30)
dim(Y)
Y<-sweep(Y,MARGIN=2,runif(30),"+")
norm <- normalizeData(Y, type="center")
norm<-normalizeData(Y,type="center")
Y <- list(Y[,1:10], Y[,11:30])
norm<-normalizeData(Y,type="center")
norm<-normalizeData(Y)
norm<-normalizeData(Y,type="center")
?normalizeData
normalizeData(Y,type="center")
paste(year(datacfs_date$FeedbackDate), "-M", month(datacfs_date$FeedbackDate),#
      sep="")
paste(year(datacfs_date$FeedbackDate), "-M", month(datacfs_date$FeedbackDate),sep="")
X <- 2
file = "X.txt"
write.table(X, paste(file, 2, sep = "."))
dir()
file = "X"
filename = paste(file, i, sep = "_")
filename = paste(file, 1, sep = "_")
filename
write.table(X, paste(filename, ".txt", sep = "."))
dir()
write.table(X, paste(filename, "txt", sep = "."))
dir()
X
X <- [1 2 3]
write.table(X, file = "abs", append = FALSE, quote = TRUE, sep = " ",#
            eol = "\n", na = NA, row.names = TRUE, col.names = TRUE)
write.table(X, file = "abs", append = FALSE, quote = TRUE, sep = " ",#
            eol = "\n", row.names = TRUE, col.names = TRUE)
write.table(X, file = "abs", append = FALSE, quote = TRUE, sep = " ",#
            eol = "\n", row.names = F, col.names = F)
y = 1
file = "av"
file0 = paste(file, 1, sep = "")
file0
dir()
opts
clc
ls
dir()
library("CCAGFA")
i < 1
i <- 1
setwd("/Users/sikun/Documents/MATLAB/code_FA/CVB_HBPFA/simulation_BASS/Four_Group/data")
inputprefix = "BASS_sim3_n50_seed"#
input = paste(inputprefix, i, sep = "")#
#
mat <- read.delim(paste(input, "txt", sep = "."),header = F)#
mat1 <- mat[,c(1:50)]#
#
# Separate datasets#
dat1 <- t(mat1[1:100,])#
dat2 <- t(mat1[101:200,])#
dat3 <- t(mat1[201:300,])#
dat4 <- t(mat1[301:400,])#
#
# Form a list file which is an input format to GFA#
comb.mat <- list(dat1,dat2,dat3,dat4)
mat <- read.delim(paste(input, "txt", sep = "."),header = F)
inputprefix = "BASS_sim3_n20_seed"
input = paste(inputprefix, i, sep = "")#
#
mat <- read.delim(paste(input, "txt", sep = "."),header = F)#
mat1 <- mat[,c(1:50)]#
#
# Separate datasets#
dat1 <- t(mat1[1:100,])#
dat2 <- t(mat1[101:200,])#
dat3 <- t(mat1[201:300,])#
dat4 <- t(mat1[301:400,])#
#
# Form a list file which is an input format to GFA#
comb.mat <- list(dat1,dat2,dat3,dat4)
mat1 <- mat[,c(1:20)]
dat1 <- t(mat1[1:100,])#
dat2 <- t(mat1[101:200,])#
dat3 <- t(mat1[201:300,])#
dat4 <- t(mat1[301:400,])
comb.mat <- list(dat1,dat2,dat3,dat4)
opts <- getDefaultOpts()
model <- GFAexperiment(comb.mat,6,opts)
model
beta_0
model <- GFAexperiment(comb.mat,6,opts)
model <- GFAexperiment(comb.mat,6,opts)
model <- GFAexperiment(comb.mat,6,opts)
help crossprod
crossprod''
crossprod
